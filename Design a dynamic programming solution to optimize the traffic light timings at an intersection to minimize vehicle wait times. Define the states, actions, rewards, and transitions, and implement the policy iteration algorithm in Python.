import itertools
states = list(itertools.product([0, 1], [0, 1], [0, 1]))
actions = [0, 1]     
gamma = 0.9          
def reward(state):
    green, ns, ew = state

    if green == 0:       
        return - (3 if ew == 1 else 1)
    else:               
        return - (3 if ns == 1 else 1)
def transitions(state, action):
    green, ns, ew = state

    if action == 1:
        green = 1 - green   #

    next_states = []
    for ns_next in [0, 1]:
        for ew_next in [0, 1]:
            next_states.append((green, ns_next, ew_next))

    prob = 1 / len(next_states)
    return [(prob, s) for s in next_states]

policy = {s: 0 for s in states}
V = {s: 0 for s in states}
policy_stable = False

while not policy_stable:
    for _ in range(50):
        for s in states:
            a = policy[s]
            V[s] = reward(s) + gamma * sum(
                p * V[s1] for p, s1 in transitions(s, a)
            )
    policy_stable = True
    for s in states:
        old_action = policy[s]

        action_values = {}
        for a in actions:
            action_values[a] = reward(s) + gamma * sum(
                p * V[s1] for p, s1 in transitions(s, a)
            )

        policy[s] = max(action_values, key=action_values.get)

        if old_action != policy[s]:
            policy_stable = False
print("Optimal Policy:")
for s in states:
    action = "Switch" if policy[s] == 1 else "Keep"
    print(f"State {s} -> {action}")
